networks:
  nginx-manager_customnetwork:
    external: true

services:
  zookeeper:
    restart: always
    image: confluentinc/cp-zookeeper
    container_name: ver-obspy-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - nginx-manager_customnetwork


  kafka:
    restart: always
    container_name: ver-obspy-kafka
    image: confluentinc/cp-kafka
    depends_on:
      - zookeeper
    ports:
      - '19092:19092'
      - '29092:29092'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'ver-obspy-zookeeper:2181'
      KAFKA_LISTENERS: INSIDE://ver-obspy-kafka:9092,OUTSIDE://0.0.0.0:19092
      KAFKA_ADVERTISED_LISTENERS: INSIDE://ver-obspy-kafka:9092,OUTSIDE://85.209.163.202:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: '1'
      KAFKA_MAX_MESSAGE_BYTES: 700000000
      KAFKA_MESSAGE_MAX_BYTES: 700000000
    networks:
      - nginx-manager_customnetwork

  init-kafka:
    container_name: ver-obspy-init-kafka
    image: confluentinc/cp-kafka:6.1.1
    depends_on:
      - kafka
      - zookeeper
    entrypoint: ['/bin/sh', '-c']
    command: |
      "
      # blocks until kafka is reachable
      kafka-topics --bootstrap-server ver-obspy-kafka:19092 --list
      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server ver-obspy-kafka:19092 --create --if-not-exists --topic seismic_preprocess --replication-factor 1 --partitions 3 --config segment.bytes=150000 --config retention.bytes=3000000 --config retention.ms=120000
      kafka-topics --bootstrap-server ver-obspy-kafka:19092 --create --if-not-exists --topic seismic_store --replication-factor 1 --partitions 3 --config segment.bytes=150000 --config retention.bytes=3000000 --config retention.ms=120000
      kafka-topics --bootstrap-server ver-obspy-kafka:19092 --create --if-not-exists --topic pick_data --replication-factor 1 --partitions 3 --config segment.bytes=150000 --config retention.bytes=3000000 --config retention.ms=120000
      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server ver-obspy-kafka:19092 --list
      "

  redis:
    image: redis
    container_name: ver-obspy-redis
    environment:
    - REDIS_PASSWORD=${REDIS_PASSWORD}
    command: redis-server --requirepass ${REDIS_PASSWORD}
    ports:
      - '6379:6379'
    volumes:
      - redis_data:/data

  seeder:
    image: confluentinc/cp-kafka
    depends_on:
      - redis
      - kafka
      - init-kafka
    build:
      context: ./picker
      dockerfile: Dockerfile
    container_name: ver-obspy-picker
    environment:
      - .seeder.env
    restart: always

  producer:
    image: confluentinc/cp-kafka
    depends_on:
      - redis
      - kafka
      - init-kafka
    build:
      context: ./producer
      dockerfile: Dockerfile
    container_name: ver-obspy-producer
    environment:
      - .producer.env
    ports:
      - '8000:8000'
    restart: always

  queue:
    image: confluentinc/cp-kafka
    depends_on:
      - producer
      - kafka
      - init-kafka
    build:
      context: ./queue
      dockerfile: Dockerfile
    container_name: ver-obspy-queue
    environment:
      - .queue.env
    restart: always
    mem_limit: 60m

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.0.1
    volumes:
      - ./data/elasticsearch:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
      - 9300:9300
    networks:
      - nginx-manager_customnetwork

  ver-obspy-mongo:
    image: mongo:latest
    container_name: ver-obspy-mongo
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_DB_NAME: ${MONGO_DB_NAME}
    volumes:
      - vertexes-stream_mongodb_data:/data/db
    networks:
      - vertexes-stream_default
    ports:
      - "27017:27017"

  # profiler:
  #   depends_on:
  #     - elasticsearch
  #   image: aurum/ddprofiler:0.1
  #   build:
  #     context: .
  #     dockerfile: docker/Dockerfile-ddprofiler
  #   volumes:
  #     - ./data:/data
  #   command: " --store.server elasticsearch --sources /data/mitdwhdata.yml"
  #   networks:
  #     aurum_net:
  #       aliases:
  #         - profiler

  # nbc:
  #   depends_on:
  #     - elasticsearch
  #   image: aurum/networkbuildercoordinator:0.1
  #   build:
  #     context: .
  #     dockerfile: docker/Dockerfile-networkbuildercoordinator
  #   volumes:
  #     - ./data/models:/output
  #   networks:
  #     aurum_net:
  #       aliases:
  #         - nbc

  # notebook:
  #   depends_on:
  #     - elasticsearch
  #   image: aurum/notebook
  #   build:
  #     context: .
  #     dockerfile: docker/Dockerfile-jupyter-notebook
  #   volumes:
  #     - ./data/models:/data/models
  #   ports:
  #     - 8888:8888
  #   networks:
  #     aurum_net:
  #       aliases:
  #         - notebook

volumes:
  redis_data:
  elasticsearch_data:
  mongodb_data:
